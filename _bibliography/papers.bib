---
---


@inproceedings{li2023transformers,
  title={Transformers as algorithms: Generalization and stability in in-context learning},
  author={Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  abbr={ICML},
  pages={19565--19594},
  year={2023},
  organization={PMLR}
}

@article{tarzanagh2023transformers,
  title={Transformers as support vector machines},
  author={Tarzanagh*, Davoud Ataee and Li*, Yingcong and Thrampoulidis, Christos and Oymak, Samet},
  abbr={in submission},
  year={2023}
}

@inproceedings{chang2021provable,
  title={Provable benefits of overparameterization in model compression: From double descent to pruning neural networks},
  author={Chang*, Xiangyu and Li*, Yingcong and Oymak, Samet and Thrampoulidis, Christos},
  abbr={AAAI},
  volume={35},
  number={8},
  pages={6974--6983},
  year={2021}
}

@article{ataee2023max,
  title={Max-margin token selection in attention mechanism},
  author={Ataee Tarzanagh, Davoud and Li, Yingcong and Zhang, Xuechen and Oymak, Samet},
  abbr={NeurIPS spotlight},
  volume={36},
  pages={48314--48362},
  year={2023}
}

@article{li2024dissecting,
  title={Dissecting chain-of-thought: Compositionality through in-context filtering and learning},
  author={Li, Yingcong and Sreenivasan, Kartik and Giannou, Angeliki and Papailiopoulos, Dimitris and Oymak, Samet},
  abbr={NeurIPS},
  volume={36},
  year={2024}
}

@inproceedings{li2024mechanics,
  title={Mechanics of next token prediction with self-attention},
  author={Li*, Yingcong and Huang*, Yixiao and Ildiz, Muhammed E and Rawat, Ankit Singh and Oymak, Samet},
  abbr={AISTATS},
  pages={685--693},
  year={2024},
  organization={PMLR}
}

@article{li2022provable,
  title={Provable and efficient continual representation learning},
  author={Li, Yingcong and Li, Mingchen and Asif, M Salman and Oymak, Samet},
  abbr={arXiv preprint},
  year={2022}
}

@article{ildiz2024self,
  title={From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers},
  author={Ildiz, M Emrullah and Huang, Yixiao and Li, Yingcong and Rawat, Ankit Singh and Oymak, Samet},
  abbr={ICML},
  year={2024}
}

@inproceedings{li2023provable,
  title={Provable pathways: Learning multiple tasks over multiple paths},
  author={Li, Yingcong and Oymak, Samet},
  abbr={AAAI},
  volume={37},
  number={7},
  pages={8701--8710},
  year={2023}
}

@inproceedings{qin2023stochastic,
  title={Stochastic contextual bandits with long horizon rewards},
  author={Qin, Yuzhen and Li, Yingcong and Pasqualetti, Fabio and Fazel, Maryam and Oymak, Samet},
  abbr={AAAI},
  volume={37},
  number={8},
  pages={9525--9533},
  year={2023}
}

@inproceedings{li2023fairness,
  title={On the fairness of multitask representation learning},
  author={Li, Yingcong and Oymak, Samet},
  abbr={ICASSP},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

@article{li2024fine,
  title={Fine-grained Analysis of In-context Linear Estimation: Data, Architecture, and Beyond},
  author={Li, Yingcong and Rawat, Ankit Singh and Oymak, Samet},
  journal={in submission},
  year={2024}
}

@inproceedings{li2024can,
  title={Can Mamba In-Context Learn Task Mixtures?},
  author={Li, Yingcong and Wei, Xupeng and Zhao, Haonan and Ma, Taigao},
  abbr={ICML Workshop},
  year={2024}
}